{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2edf2fec-54b9-4f62-acf7-dc141693cbb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipynb in c:\\users\\mustafa\\anaconda3\\lib\\site-packages (0.5.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install ipynb\n",
    "\n",
    "# Run the above line to get the preprocess() function from Preprocessing.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff5f5eb7-5ee9-4bf2-807c-9e137e044520",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "from ipynb.fs.full.Preprocessing import preprocess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d297fb8d-c53a-437a-894e-c4ccd29bcf22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_corrector(array):\n",
    "    temp = list()\n",
    "    seen_labels = set()\n",
    "    dict_of_labels = dict()\n",
    "    label_counter = -1\n",
    "    for label in array:\n",
    "        if label in seen_labels:\n",
    "            temp.append(dict_of_labels[label])\n",
    "        else:\n",
    "            label_counter += 1\n",
    "            dict_of_labels[label] = label_counter\n",
    "            seen_labels.add(label)\n",
    "            temp.append(label_counter)\n",
    "    return np.array(temp)\n",
    "\n",
    "def k_fold_preprocess(row_count=100000, k=10):\n",
    "    \"\"\"\n",
    "    Splits the dataset into k groups\n",
    "    \n",
    "    Output: [ ((validation_1_x, validation_1_y), (training_1_x, training_1_y)),\n",
    "              ((validation_2_x, validation_2_y), (training_2_x, training_2_y)),\n",
    "              ...\n",
    "              ((validation_k_x, validation_k_y), (training_k_x, training_k_y))]\n",
    "    \"\"\"\n",
    "    df = pd.read_csv('star_classification.csv')\n",
    "    df.drop(['obj_ID'], axis = 1,inplace=True) #dropping OBJ_ID feature as it is just the ID\n",
    "    df.drop(['rerun_ID'], axis = 1,inplace=True)#dropping rerun_ID feature as it is the same for all\n",
    "    df.drop(['run_ID'], axis = 1,inplace=True)\n",
    "    df.drop(['field_ID'], axis = 1,inplace=True)\n",
    "    df.drop(['spec_obj_ID'], axis = 1,inplace=True)\n",
    "    \n",
    "    data = np.array(df)\n",
    "    class_index = 8\n",
    "    data[:,class_index] = label_corrector(data[:,class_index])\n",
    "    \n",
    "    complete_x = np.concatenate((data[:,:class_index],data[:,class_index+1:]),axis=1)\n",
    "    complete_y = data[:,class_index]\n",
    "    \n",
    "    # Remove outliers from the dataset\n",
    "    tdata = np.transpose(complete_x)\n",
    "    \n",
    "    IQRs = list()\n",
    "    outlier_rows = np.array([])\n",
    "    for ind, col in  enumerate(tdata):\n",
    "        if ind in [3,4,5,6,2]:\n",
    "            percentiles = np.percentile(col,[25,75])\n",
    "            IQR = percentiles[1]-percentiles[0]\n",
    "            right = (col>percentiles[1]+1.5*IQR)*1\n",
    "            left = (col < (percentiles[0]-1.5*IQR))*1\n",
    "            outliers = right+left\n",
    "            #print(f\"{ind}th feature has {sum(outliers)} outliers.\")\n",
    "            outlier_rows = np.append(outlier_rows,np.where(outliers == 1)[0])\n",
    "            \n",
    "    outlier_rows = np.array(list(set(outlier_rows)))\n",
    "    outlier_rows = np.array(outlier_rows,dtype = 'int')\n",
    "    \n",
    "    complete_x = np.delete(complete_x,outlier_rows,axis=0)\n",
    "    complete_y = np.delete(complete_y,outlier_rows,axis=0)\n",
    "    \n",
    "    # Standardize the dataset\n",
    "    means_x = list()\n",
    "    stds_x = list()\n",
    "\n",
    "    for ind in range(np.shape(complete_x)[1]):\n",
    "        col = complete_x[:,ind]\n",
    "        mean = np.mean(col)\n",
    "        std = np.std(col)\n",
    "        means_x.append(mean)\n",
    "        stds_x.append(std)\n",
    "        complete_x[:,ind] = (col-mean)/std\n",
    "        \n",
    "    \"\"\"\n",
    "    Splits the dataset into k groups\n",
    "    \n",
    "    Output: [ ((validation_1_x, validation_1_y), (training_1_x, training_1_y)),\n",
    "              ((validation_2_x, validation_2_y), (training_2_x, training_2_y)),\n",
    "              ...\n",
    "              ((validation_k_x, validation_k_y), (training_k_x, training_k_y))]\n",
    "    \"\"\"\n",
    "        \n",
    "    folds = []\n",
    "    fold_size = complete_x.shape[0] // k\n",
    "    prev_index = 0\n",
    "    for i in range(k):\n",
    "        validation_x = complete_x[prev_index:prev_index + fold_size]\n",
    "        validation_y = complete_y[prev_index:prev_index + fold_size]\n",
    "        \n",
    "        training_x = np.concatenate((complete_x[0:prev_index], complete_x[prev_index+fold_size:]), axis=0)\n",
    "        training_y = np.concatenate((complete_y[0:prev_index], complete_y[prev_index+fold_size:]), axis=0)\n",
    "        \n",
    "        folds.append( ((validation_x, validation_y), (training_x, training_y)) )\n",
    "        prev_index += fold_size\n",
    "        \n",
    "        \n",
    "    return folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc71cfb7-959d-4f86-923f-f0d3506b4ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LogisticRegressionTrain(train_x, train_y, lr=0.5):\n",
    "    number_of_classes = 3\n",
    "    number_of_features = train_x.shape[1]\n",
    "    number_of_samples = train_x.shape[0]\n",
    "    learning_rate = lr\n",
    "    \n",
    "    assert(number_of_features == 12)\n",
    "    \n",
    "    # One-hot encode the classes for training\n",
    "    # E.g., if class array is [0, 0, 1, 0, 2]\n",
    "    # Convert to [[1,0,0], [1,0,0], [0,1,0], [1,0,0],[0,0,1]]\n",
    "    one_hot_encoded = []\n",
    "\n",
    "    for arr in (train_y):\n",
    "        if (arr == [0]):\n",
    "            one_hot_encoded.append([1, 0, 0,])\n",
    "        elif (arr == [1]):\n",
    "            one_hot_encoded.append([0, 1, 0,])\n",
    "        elif (arr == [2]):\n",
    "            one_hot_encoded.append([0, 0, 1,])\n",
    "        else:\n",
    "            print(\"error\")\n",
    "\n",
    "    encoded_train_y = np.array(one_hot_encoded)\n",
    "\n",
    "    # Initialize the weights\n",
    "    weights = np.random.rand(number_of_features, number_of_classes)\n",
    "    bias = np.random.randn(1, number_of_classes)\n",
    "    \n",
    "    iteration_count = 1000\n",
    "    for i in range(1, iteration_count + 1):\n",
    "        # Get the Log-odds (logit)\n",
    "        logits = train_x @ weights + bias\n",
    "\n",
    "        # Calculate the probability of y \n",
    "        # Pr(Y = 0, 1, or 2 | X = train_x)\n",
    "        max_value = np.max(logits, axis =1, keepdims=True)\n",
    "        numerator = np.exp(logits - max_value)\n",
    "        denominator = np.sum(numerator, axis=-1, keepdims=True)\n",
    "        y_probability = numerator / denominator\n",
    "                \n",
    "        # Calculate the gradient vectors \n",
    "        \n",
    "        # Gradient of the weights\n",
    "        dW = (1 / number_of_samples) * (train_x.T @ (y_probability - encoded_train_y))\n",
    "        \n",
    "        # Gradient of the bias\n",
    "        db = (1 / number_of_samples) * np.sum((y_probability - encoded_train_y), axis=0, keepdims=True)\n",
    "        \n",
    "        # We use gradient descent to update the weights and bias because\n",
    "        # the number of samples is quite high. If we had used Newton-Raphson \n",
    "        # method, we would have had to compute the inverse of 100,000x100,000\n",
    "        # matrix which is impossible due to hardware insufficiencies.\n",
    "        weights -= (dW * learning_rate)\n",
    "        bias -= (db * learning_rate)\n",
    "        \n",
    "    return weights, bias\n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3236c69e-779e-49c3-80d4-f4d559dd827b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prediction(feature, weights, bias, threshold=0.5):\n",
    "    logits = feature @ weights + bias\n",
    "\n",
    "    max_value = np.max(logits, axis =1, keepdims=True)\n",
    "    numerator = np.exp(logits - max_value)\n",
    "    denominator = np.sum(numerator, axis=-1, keepdims=True)\n",
    "    y_probability = numerator / denominator\n",
    "    \n",
    "    return np.argmax(y_probability, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "208b4b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sn\n",
    "\n",
    "def confusion_matrix(test_x, test_y, weights, bias, verbose=False):\n",
    "    prediction = make_prediction(test_x, weights, bias)    \n",
    "    confusion_matrix = np.zeros((3, 3), dtype=int)\n",
    "    \n",
    "    accurate_prediction_count = 0\n",
    "    for i, pred in enumerate(prediction):\n",
    "        if test_y[i][0] == 0:\n",
    "            if pred == 0: \n",
    "                confusion_matrix[0][0] += 1\n",
    "            elif pred == 1:\n",
    "                confusion_matrix[0][1] += 1\n",
    "            elif pred == 2:\n",
    "                confusion_matrix[0][2] += 1\n",
    "        \n",
    "        elif test_y[i][0] == 1:\n",
    "            if pred == 0:\n",
    "                confusion_matrix[1][0] += 1\n",
    "            elif pred == 1:\n",
    "                confusion_matrix[1][1] += 1\n",
    "            elif pred == 2:\n",
    "                confusion_matrix[1][2] += 1\n",
    "                \n",
    "        elif test_y[i][0] == 2:\n",
    "            if pred == 0:\n",
    "                confusion_matrix[2][0] += 1\n",
    "            elif pred == 1:\n",
    "                confusion_matrix[2][1] += 1\n",
    "            elif pred == 2:\n",
    "                confusion_matrix[2][2] += 1\n",
    "        \n",
    "        if pred == test_y[i][0]:\n",
    "            accurate_prediction_count += 1\n",
    "            \n",
    "    \n",
    "    if verbose:\n",
    "        s = sn.heatmap(confusion_matrix, annot=True, fmt='g')\n",
    "        s.set(xlabel='Predicted Labels', ylabel='True Labels')\n",
    "        plt.show()\n",
    "    return accurate_prediction_count / test_y.shape[0] * 100 \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0125b16e-0dd3-4604-896a-2467ff7b0a6f",
   "metadata": {},
   "source": [
    "### Tuning the learning rate K-Fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55488b76-c8c8-44cd-b7bd-40cd8a3cd9f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy on fold 0 when learning rate = 0.0001: 51.15461847389559\n",
      "Validation accuracy on fold 1 when learning rate = 0.0001: 55.16064257028113\n",
      "Validation accuracy on fold 2 when learning rate = 0.0001: 21.997991967871485\n",
      "Validation accuracy on fold 3 when learning rate = 0.0001: 26.556224899598398\n",
      "Validation accuracy on fold 4 when learning rate = 0.0001: 34.78915662650602\n",
      "Validation accuracy on fold 5 when learning rate = 0.0001: 59.27710843373494\n",
      "Validation accuracy on fold 6 when learning rate = 0.0001: 10.522088353413654\n",
      "Validation accuracy on fold 7 when learning rate = 0.0001: 13.94578313253012\n",
      "Validation accuracy on fold 8 when learning rate = 0.0001: 56.5863453815261\n",
      "Validation accuracy on fold 9 when learning rate = 0.0001: 29.136546184738954\n",
      "Average accuracy when learning rate is 0.0001: 35.912650602409634\n",
      "\n",
      "\n",
      "\n",
      "Validation accuracy on fold 0 when learning rate = 0.001: 41.40562248995984\n",
      "Validation accuracy on fold 1 when learning rate = 0.001: 60.21084337349397\n",
      "Validation accuracy on fold 2 when learning rate = 0.001: 37.07831325301205\n",
      "Validation accuracy on fold 3 when learning rate = 0.001: 56.62650602409639\n",
      "Validation accuracy on fold 4 when learning rate = 0.001: 57.33935742971887\n",
      "Validation accuracy on fold 5 when learning rate = 0.001: 57.76104417670683\n",
      "Validation accuracy on fold 6 when learning rate = 0.001: 43.734939759036145\n",
      "Validation accuracy on fold 7 when learning rate = 0.001: 26.947791164658636\n",
      "Validation accuracy on fold 8 when learning rate = 0.001: 51.48594377510041\n",
      "Validation accuracy on fold 9 when learning rate = 0.001: 48.47389558232932\n",
      "Average accuracy when learning rate is 0.001: 48.106425702811244\n",
      "\n",
      "\n",
      "\n",
      "Validation accuracy on fold 0 when learning rate = 0.01: 84.09638554216868\n",
      "Validation accuracy on fold 1 when learning rate = 0.01: 80.38152610441767\n",
      "Validation accuracy on fold 2 when learning rate = 0.01: 83.68473895582329\n",
      "Validation accuracy on fold 3 when learning rate = 0.01: 80.90361445783132\n",
      "Validation accuracy on fold 4 when learning rate = 0.01: 82.28915662650603\n",
      "Validation accuracy on fold 5 when learning rate = 0.01: 82.41967871485943\n",
      "Validation accuracy on fold 6 when learning rate = 0.01: 82.86144578313252\n",
      "Validation accuracy on fold 7 when learning rate = 0.01: 82.38955823293172\n",
      "Validation accuracy on fold 8 when learning rate = 0.01: 76.6867469879518\n",
      "Validation accuracy on fold 9 when learning rate = 0.01: 82.37951807228916\n",
      "Average accuracy when learning rate is 0.01: 81.80923694779116\n",
      "\n",
      "\n",
      "\n",
      "Validation accuracy on fold 0 when learning rate = 0.1: 90.26104417670683\n",
      "Validation accuracy on fold 1 when learning rate = 0.1: 88.91566265060241\n",
      "Validation accuracy on fold 2 when learning rate = 0.1: 89.39759036144578\n",
      "Validation accuracy on fold 3 when learning rate = 0.1: 90.2710843373494\n",
      "Validation accuracy on fold 4 when learning rate = 0.1: 89.5281124497992\n",
      "Validation accuracy on fold 5 when learning rate = 0.1: 89.74899598393574\n",
      "Validation accuracy on fold 6 when learning rate = 0.1: 90.32128514056225\n",
      "Validation accuracy on fold 7 when learning rate = 0.1: 90.5421686746988\n",
      "Validation accuracy on fold 8 when learning rate = 0.1: 88.29317269076306\n",
      "Validation accuracy on fold 9 when learning rate = 0.1: 90.51204819277109\n",
      "Average accuracy when learning rate is 0.1: 89.77911646586347\n",
      "\n",
      "\n",
      "\n",
      "Validation accuracy on fold 0 when learning rate = 0.5: 93.3132530120482\n",
      "Validation accuracy on fold 1 when learning rate = 0.5: 92.96184738955823\n",
      "Validation accuracy on fold 2 when learning rate = 0.5: 92.89156626506025\n",
      "Validation accuracy on fold 3 when learning rate = 0.5: 93.84538152610442\n",
      "Validation accuracy on fold 4 when learning rate = 0.5: 93.21285140562249\n",
      "Validation accuracy on fold 5 when learning rate = 0.5: 93.14257028112449\n",
      "Validation accuracy on fold 6 when learning rate = 0.5: 93.57429718875501\n",
      "Validation accuracy on fold 7 when learning rate = 0.5: 93.97590361445783\n",
      "Validation accuracy on fold 8 when learning rate = 0.5: 92.27911646586345\n",
      "Validation accuracy on fold 9 when learning rate = 0.5: 93.33333333333333\n",
      "Average accuracy when learning rate is 0.5: 93.25301204819277\n",
      "\n",
      "\n",
      "\n",
      "Validation accuracy on fold 0 when learning rate = 0.7: 93.72489959839358\n",
      "Validation accuracy on fold 1 when learning rate = 0.7: 93.52409638554217\n",
      "Validation accuracy on fold 2 when learning rate = 0.7: 93.35341365461848\n",
      "Validation accuracy on fold 3 when learning rate = 0.7: 94.13654618473896\n",
      "Validation accuracy on fold 4 when learning rate = 0.7: 93.76506024096386\n",
      "Validation accuracy on fold 5 when learning rate = 0.7: 93.68473895582329\n",
      "Validation accuracy on fold 6 when learning rate = 0.7: 93.83534136546184\n",
      "Validation accuracy on fold 7 when learning rate = 0.7: 94.50803212851405\n",
      "Validation accuracy on fold 8 when learning rate = 0.7: 92.85140562248996\n",
      "Validation accuracy on fold 9 when learning rate = 0.7: 93.75502008032129\n",
      "Average accuracy when learning rate is 0.7: 93.71385542168676\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Splits the dataset into k groups\n",
    "    \n",
    "    Output: [ ((validation_1_x, validation_1_y), (training_1_x, training_1_y)),\n",
    "              ((validation_2_x, validation_2_y), (training_2_x, training_2_y)),\n",
    "              ...\n",
    "              ((validation_k_x, validation_k_y), (training_k_x, training_k_y))]\n",
    "\"\"\"\n",
    "k = 10\n",
    "folds = k_fold_preprocess(100000, 10)\n",
    "best_accuracy = 0\n",
    "\n",
    "learning_rates = [0.0001, 0.001, 0.01, 0.1, 0.5, 0.7]\n",
    "average_accuracies = []\n",
    "\n",
    "for lr in learning_rates:   \n",
    "    acc_rate = 0\n",
    "    for ind in range(k):\n",
    "        validation_x = folds[ind][0][0]\n",
    "        validation_y = folds[ind][0][1]\n",
    "\n",
    "        training_x = folds[ind][1][0]\n",
    "        training_y = folds[ind][1][1]\n",
    "\n",
    "        training_y = training_y.reshape(-1, 1)\n",
    "        training_y = training_y.astype('int')\n",
    "\n",
    "        training_x = training_x.astype('float')\n",
    "        validation_x = validation_x.astype('float')\n",
    "\n",
    "        validation_y = validation_y.reshape(-1, 1)\n",
    "        validation_y = validation_y.astype('int')\n",
    "\n",
    "        weights, bias = LogisticRegressionTrain(training_x, training_y, lr=lr)\n",
    "        acc = confusion_matrix(validation_x, validation_y, weights, bias)\n",
    "        \n",
    "        print(f\"Validation accuracy on fold {ind} when learning rate = {lr}: {acc}\")\n",
    "        \n",
    "        acc_rate += acc\n",
    "    average_accuracies.append(acc_rate / k)\n",
    "    print(f\"Average accuracy when learning rate is {lr}: {acc_rate / k}\")\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe2e7c0b-b46d-4226-8286-9aaef21669a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(learning_rates, average_accuracies)\n",
    "plt.xlabel(\"Learning rate\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19908c9-bf17-49bf-9024-5c1025a39dc2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
