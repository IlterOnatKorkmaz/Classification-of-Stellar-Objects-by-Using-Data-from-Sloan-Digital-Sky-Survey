{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc6b4ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "from ipynb.fs.full.Preprocessing import preprocess\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd783375-4557-4bef-96f6-b6fb90e340c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_corrector(array):\n",
    "    temp = list()\n",
    "    seen_labels = set()\n",
    "    dict_of_labels = dict()\n",
    "    label_counter = -1\n",
    "    for label in array:\n",
    "        if label in seen_labels:\n",
    "            temp.append(dict_of_labels[label])\n",
    "        else:\n",
    "            label_counter += 1\n",
    "            dict_of_labels[label] = label_counter\n",
    "            seen_labels.add(label)\n",
    "            temp.append(label_counter)\n",
    "    return np.array(temp)\n",
    "\n",
    "def k_fold_preprocess(row_count=100000, k=10):\n",
    "    \"\"\"\n",
    "    Splits the dataset into k groups\n",
    "    \n",
    "    Output: [ ((validation_1_x, validation_1_y), (training_1_x, training_1_y)),\n",
    "              ((validation_2_x, validation_2_y), (training_2_x, training_2_y)),\n",
    "              ...\n",
    "              ((validation_k_x, validation_k_y), (training_k_x, training_k_y))]\n",
    "    \"\"\"\n",
    "    df = pd.read_csv('star_classification.csv')\n",
    "    df.drop(['obj_ID'], axis = 1,inplace=True) #dropping OBJ_ID feature as it is just the ID\n",
    "    df.drop(['rerun_ID'], axis = 1,inplace=True)#dropping rerun_ID feature as it is the same for all\n",
    "    df.drop(['run_ID'], axis = 1,inplace=True)\n",
    "    df.drop(['field_ID'], axis = 1,inplace=True)\n",
    "    df.drop(['spec_obj_ID'], axis = 1,inplace=True)\n",
    "    \n",
    "    data = np.array(df)\n",
    "    data = data[:row_count]\n",
    "    class_index = 8\n",
    "    data[:,class_index] = label_corrector(data[:,class_index])\n",
    "    \n",
    "    complete_x = np.concatenate((data[:,:class_index],data[:,class_index+1:]),axis=1)\n",
    "    complete_y = data[:,class_index]\n",
    "    \n",
    "    # Remove outliers from the dataset\n",
    "    tdata = np.transpose(complete_x)\n",
    "    \n",
    "    IQRs = list()\n",
    "    outlier_rows = np.array([])\n",
    "    for ind, col in  enumerate(tdata):\n",
    "        if ind in [3,4,5,6,2]:\n",
    "            percentiles = np.percentile(col,[25,75])\n",
    "            IQR = percentiles[1]-percentiles[0]\n",
    "            right = (col>percentiles[1]+1.5*IQR)*1\n",
    "            left = (col < (percentiles[0]-1.5*IQR))*1\n",
    "            outliers = right+left\n",
    "            #print(f\"{ind}th feature has {sum(outliers)} outliers.\")\n",
    "            outlier_rows = np.append(outlier_rows,np.where(outliers == 1)[0])\n",
    "            \n",
    "    outlier_rows = np.array(list(set(outlier_rows)))\n",
    "    outlier_rows = np.array(outlier_rows,dtype = 'int')\n",
    "    \n",
    "    complete_x = np.delete(complete_x,outlier_rows,axis=0)\n",
    "    complete_y = np.delete(complete_y,outlier_rows,axis=0)\n",
    "    \n",
    "    # Standardize the dataset\n",
    "    means_x = list()\n",
    "    stds_x = list()\n",
    "\n",
    "    for ind in range(np.shape(complete_x)[1]):\n",
    "        col = complete_x[:,ind]\n",
    "        mean = np.mean(col)\n",
    "        std = np.std(col)\n",
    "        means_x.append(mean)\n",
    "        stds_x.append(std)\n",
    "        complete_x[:,ind] = (col-mean)/std\n",
    "        \n",
    "    \"\"\"\n",
    "    Splits the dataset into k groups\n",
    "    \n",
    "    Output: [ ((validation_1_x, validation_1_y), (training_1_x, training_1_y)),\n",
    "              ((validation_2_x, validation_2_y), (training_2_x, training_2_y)),\n",
    "              ...\n",
    "              ((validation_k_x, validation_k_y), (training_k_x, training_k_y))]\n",
    "    \"\"\"\n",
    "        \n",
    "    folds = []\n",
    "    fold_size = complete_x.shape[0] // k\n",
    "    prev_index = 0\n",
    "    for i in range(k):\n",
    "        validation_x = complete_x[prev_index:prev_index + fold_size]\n",
    "        validation_y = complete_y[prev_index:prev_index + fold_size]\n",
    "        \n",
    "        training_x = np.concatenate((complete_x[0:prev_index], complete_x[prev_index+fold_size:]), axis=0)\n",
    "        training_y = np.concatenate((complete_y[0:prev_index], complete_y[prev_index+fold_size:]), axis=0)\n",
    "        \n",
    "        folds.append( ((validation_x, validation_y), (training_x, training_y)) )\n",
    "        prev_index += fold_size\n",
    "        \n",
    "        \n",
    "    return folds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821153c1",
   "metadata": {},
   "source": [
    "# Decision Tree Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "588e29e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_x, train_y, test_x, test_y = preprocess(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b034a627",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This one uses Counter\n",
    "class Node:\n",
    "    #The tree is defined with nodes that have nodes in them. The process will be implemented recursively \n",
    "    #in later parts.\n",
    "    \n",
    "    def __init__(self, depth=None,right_node=None, left_node=None,node_type=None,node_class = None,split_feature = None,split_threshold = None ):\n",
    "        #The building block of the tree.\n",
    "        #The tree is a node that have nodes...\n",
    "        self.left_node = left_node\n",
    "        self.right_node =right_node\n",
    "        self.node_type = node_type\n",
    "        self.node_class = node_class\n",
    "        self.split_feature = split_feature\n",
    "        self.split_threshold = split_threshold\n",
    "\n",
    "class DecisionTree:\n",
    "    def __init__(self, data=None, labels=None,max_split =500,min_member=5):\n",
    "        self.max_split=max_split\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.min_member=min_member\n",
    "        \n",
    "    def get_class(self,labels):\n",
    "        unique, counts = np.unique(labels, return_counts=True)\n",
    "        counter = dict(zip(unique, counts))\n",
    "        return max(labels, key=lambda x: counter[x])\n",
    "\n",
    "    def run(self,data,labels,split_number):\n",
    "\n",
    "        #This is the function where the recursion happens: nodes that have nodes.\n",
    "       \n",
    "        split_feature,split_threshold,gini = self.get_splitting_point(data,labels) #returns the feature index and the threshold\n",
    "        if gini > 0 and split_number< self.max_split and len(labels) > self.min_member: #split until spliting conditions are met\n",
    "            right_data, left_data,right_labels,left_labels = self.get_split(split_feature, split_threshold, data,labels)\n",
    "            right_node = self.run(data=right_data,labels = right_labels,split_number=split_number+1) \n",
    "            left_node = self.run(data = left_data,labels=left_labels,split_number=split_number+1)\n",
    "            return Node(node_type = \"branch\",right_node = right_node, left_node = left_node,split_feature=split_feature,split_threshold=split_threshold,depth=split_number  )\n",
    "        else:\n",
    "            return Node(node_type = \"leaf\",node_class = self.get_class(labels))\n",
    "    def get_splitting_point(self,data,labels):\n",
    "        num_instances, num_features = np.shape(data)\n",
    "        gini =np.inf\n",
    "        for feature in range(num_features):\n",
    "            split_threshold, gini_gain = self.best_of_feature_split(data,labels,feature)\n",
    "            if gini_gain<gini:\n",
    "                gini = gini_gain\n",
    "                best_split_threshold , best_split_feature = split_threshold,feature\n",
    "        return best_split_feature,best_split_threshold,gini \n",
    "        #return split_feature, split_threshold, gini_gain\n",
    "    def best_of_feature_split(self,data,labels,split_feature):\n",
    "        feature_col = data[:,split_feature]\n",
    "        thresholds = np.unique(feature_col, return_counts=False) #using the values as thresholds to make it efficient\n",
    "        min_gini = np.inf \n",
    "        for threshold in thresholds:\n",
    "            right_data, left_data,right_labels,left_labels = self.get_split(split_feature,threshold,data,labels)\n",
    "            len_right = len(right_labels)\n",
    "            len_left = len(left_labels)\n",
    "            gini = self.gini_index(right_labels)*(len_right/(len_right+len_left))+self.gini_index(left_labels)*(len_left/(len_right+len_left))\n",
    "            if gini < min_gini:\n",
    "                min_gini = gini\n",
    "                split_threshold = threshold\n",
    "        return split_threshold, min_gini\n",
    "    def get_split(self,split_feature,split_threshold,data,labels):\n",
    "        feature_col = data[:,split_feature]\n",
    "        pos1 =np.where(feature_col>split_threshold)[0]\n",
    "        pos2 = np.where(feature_col<=split_threshold)[0]\n",
    "        right_data = data[pos1]\n",
    "        left_data=data[pos2]\n",
    "        left_labels=labels[pos2] \n",
    "        right_labels=labels[pos1]  \n",
    "        return right_data, left_data,right_labels,left_labels   \n",
    "    def fit(self):\n",
    "        self.decision_maker = self.run(self.data,self.labels,0) #this returns a node instance \n",
    "    def predict(self,new_data):\n",
    "        predictions = list()\n",
    "        for row in new_data:\n",
    "            predictions.append(self.infer(self.decision_maker,row))\n",
    "        return predictions\n",
    "            \n",
    "    def infer(self,node,datapoint):\n",
    "        if node.node_type == \"branch\":\n",
    "            val = datapoint[node.split_feature] \n",
    "            if val > node.split_threshold:\n",
    "                return self.infer(node.right_node,datapoint)\n",
    "            else:\n",
    "                return self.infer(node.left_node,datapoint)\n",
    "        elif node.node_type == \"leaf\":\n",
    "                return node.node_class\n",
    "    def gini_index(self,labels):\n",
    "        length = len(labels)\n",
    "\n",
    "        counter = Counter(labels)\n",
    "        gini = 1\n",
    "        for val in counter.values():\n",
    "            gini -= (val/length)**2\n",
    "        return gini\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f3607b",
   "metadata": {},
   "source": [
    "**Training and Inference Time**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e52cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "data_count = 2000\n",
    "\n",
    "k = 10\n",
    "folds = k_fold_preprocess(data_count, k)\n",
    "\n",
    "for i in range(k):\n",
    "    test_x = folds[i][0][0]\n",
    "    test_y = folds[i][0][1]\n",
    "    \n",
    "    train_x = folds[i][1][0]\n",
    "    train_y = folds[i][1][1]\n",
    "    \n",
    "    print(test_y.shape)\n",
    "    \n",
    "    Tree = DecisionTree(train_x,train_y,max_split =4)\n",
    "    t1 = time()\n",
    "    Tree.fit()\n",
    "   \n",
    "    predictions = Tree.predict(test_x)\n",
    "    \n",
    "    count = 0\n",
    "    for ind,prediction in enumerate(predictions):\n",
    "        if full_test_label[ind]==prediction:\n",
    "            count+=1\n",
    "    print(f\"On fold: {i}, accuracy: {(count/len(predictions))*100}, training time {time()-t1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891eec70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "78e4bbfd",
   "metadata": {},
   "source": [
    "**Evaluation Metrics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802eea8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrice(pred,truth,num_of_class):\n",
    "    \"\"\"\n",
    "    Return the confusion matrix where the column indexes are predictions and \n",
    "    row indexes are ground truths.\n",
    "    \n",
    "    \"\"\"\n",
    "    confusion_mtrx=np.zeros((num_of_class,num_of_class),dtype=np.int32)\n",
    "    for ind,prediction in enumerate(predictions):\n",
    "        confusion_mtrx[truth[ind]][prediction]+=1\n",
    "    \n",
    "    #dataframe = pd.DataFrame(data=confusion_mtrx,   index=np.arange(0,num_of_class),   columns=np.arange(0,num_of_class))\n",
    "    #dataframe = dataframe.style.set_caption('The Confusion Matrix ')\n",
    "    #return dataframe\n",
    "    return confusion_mtrx\n",
    "def accuracy(pred,truth):\n",
    "    count = 0\n",
    "\n",
    "    for ind,prediction in enumerate(predictions):\n",
    "        if truth[ind]==prediction:\n",
    "            count+=1\n",
    "    accuracy = (count/len(predictions))*100\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad2b8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy(predictions, full_test_label))\n",
    "confusion_matrice(predictions, full_test_label,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e84c9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = sn.heatmap(confusion_matrice(predictions, full_test_label,3), annot=True, fmt='g')\n",
    "s.set(xlabel='Predicted Labels', ylabel='True Labels')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5665dc82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b5f7ef3e",
   "metadata": {},
   "source": [
    " **Performance Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde45237",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tree = DecisionTree(train_x[:1000],train_y[:1000])\n",
    "from line_profiler import LineProfiler\n",
    "lp = LineProfiler()\n",
    "lp.enable_by_count()\n",
    "lp.add_function(Tree.best_of_feature_split)\n",
    "lp.add_function(Tree.get_split)\n",
    "lp.add_function(Tree.get_splitting_point)\n",
    "lp.add_function(Tree.run)\n",
    "lp.add_function(Tree.gini_index)\n",
    "lp_wrapper = lp(Tree.fit)\n",
    "Tree.fit()\n",
    "lp.print_stats()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "51db195162f82df05c60f2be968350518dfdf1445cc2d24d16ac47e24cb84d87"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
